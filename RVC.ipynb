{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Install to Google Drive (for Resuming Training & Automatic Saving)\n",
        "#@markdown <small> This Notebook is based on another found in: https://github.com/ardha27/AI-Song-Cover-RVC < Visit this repo to read more and support.\n",
        "%cd /content\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from IPython.display import clear_output\n",
        "from ipywidgets import Button\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"Your drive is not mounted. Creating Fake Drive.\")\n",
        "    os.makedirs('/content/drive/MyDrive')\n",
        "source = \"Rejekts\"\n",
        "!wget https://huggingface.co/{source}/project/resolve/main/project-main.zip -O '/content/project-main.zip' && unzip -n 'project-main.zip' -d /content/drive/MyDrive\n",
        "!cd '/content/drive/MyDrive/project-main' && python download_files.py && pip install -r 'requirements-safe.txt'\n",
        "!pip install pyngrok\n",
        "!rm /content/project-main.zip\n",
        "!rm -r /content/sample_data\n",
        "!mkdir -p /content/dataset\n",
        "clear_output()\n",
        "Button(description=\"\\u2714 Success\", button_style=\"success\")"
      ],
      "metadata": {
        "id": "Sb5fzhzEXK8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.Preprocess Data\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "model_name = 'elon' #{type:\"string\"}\n",
        "# <small> Enter the path to your dataset folder (a folder with audios of the vocals you will train on), or if you want just upload the audios using the File Manager into the 'dataset' folder.\n",
        "dataset_folder = '/content/drive/MyDrive/dataset'\n",
        "while len(os.listdir(dataset_folder)) < 1:\n",
        "    input(\"Your dataset folder is empty.\")\n",
        "!mkdir -p ./logs/{model_name}\n",
        "with open(f'./logs/{model_name}/preprocess.log','w') as f:\n",
        "    print(\"Starting...\")\n",
        "!python infer/modules/train/preprocess.py {dataset_folder} 40000 2 ./logs/{model_name} False 3.0 > /dev/null 2>&1\n",
        "with open(f'./logs/{model_name}/preprocess.log','r') as f:\n",
        "    if 'end preprocess' in f.read():\n",
        "        clear_output()\n",
        "        display(Button(description=\"\\u2714 Success\", button_style=\"success\"))\n",
        "    else:\n",
        "        print(\"Error preprocessing data... Make sure your dataset folder is correct.\")"
      ],
      "metadata": {
        "id": "w4wXvoez9Rce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.Extract Features\n",
        "f0method = \"rmvpe_gpu\" #  [\"pm\", \"harvest\", \"rmvpe\", \"rmvpe_gpu\"]\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "with open(f'./logs/{model_name}/extract_f0_feature.log','w') as f:\n",
        "    print(\"Starting...\")\n",
        "if f0method != \"rmvpe_gpu\":\n",
        "    !python infer/modules/train/extract/extract_f0_print.py ./logs/{model_name} 2 {f0method}\n",
        "else:\n",
        "    !python infer/modules/train/extract/extract_f0_rmvpe.py 1 0 0 ./logs/{model_name} True\n",
        "!python infer/modules/train/extract_feature_print.py cuda:0 1 0 0 ./logs/{model_name} v2\n",
        "with open(f'./logs/{model_name}/extract_f0_feature.log','r') as f:\n",
        "    if 'all-feature-done' in f.read():\n",
        "        clear_output()\n",
        "        display(Button(description=\"\\u2714 Success\", button_style=\"success\"))\n",
        "    else:\n",
        "        print(\"Error preprocessing data... Make sure your data was preprocessed.\")"
      ],
      "metadata": {
        "id": "G0MEhFM19Vq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.Train Index\n",
        "import numpy as np\n",
        "import faiss\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "def train_index(exp_dir1, version19):\n",
        "    exp_dir = \"logs/%s\" % (exp_dir1)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "    feature_dir = (\n",
        "        \"%s/3_feature256\" % (exp_dir)\n",
        "        if version19 == \"v1\"\n",
        "        else \"%s/3_feature768\" % (exp_dir)\n",
        "    )\n",
        "    if not os.path.exists(feature_dir):\n",
        "        return \"请先进行特征提取!\"\n",
        "    listdir_res = list(os.listdir(feature_dir))\n",
        "    if len(listdir_res) == 0:\n",
        "        return \"请先进行特征提取！\"\n",
        "    infos = []\n",
        "    npys = []\n",
        "    for name in sorted(listdir_res):\n",
        "        phone = np.load(\"%s/%s\" % (feature_dir, name))\n",
        "        npys.append(phone)\n",
        "    big_npy = np.concatenate(npys, 0)\n",
        "    big_npy_idx = np.arange(big_npy.shape[0])\n",
        "    np.random.shuffle(big_npy_idx)\n",
        "    big_npy = big_npy[big_npy_idx]\n",
        "    if big_npy.shape[0] > 2e5:\n",
        "        infos.append(\"Trying doing kmeans %s shape to 10k centers.\" % big_npy.shape[0])\n",
        "        yield \"\\n\".join(infos)\n",
        "        try:\n",
        "            big_npy = (\n",
        "                MiniBatchKMeans(\n",
        "                    n_clusters=10000,\n",
        "                    verbose=True,\n",
        "                    batch_size=256 * config.n_cpu,\n",
        "                    compute_labels=False,\n",
        "                    init=\"random\",\n",
        "                )\n",
        "                .fit(big_npy)\n",
        "                .cluster_centers_\n",
        "            )\n",
        "        except:\n",
        "            info = traceback.format_exc()\n",
        "            logger.info(info)\n",
        "            infos.append(info)\n",
        "            yield \"\\n\".join(infos)\n",
        "\n",
        "    np.save(\"%s/total_fea.npy\" % exp_dir, big_npy)\n",
        "    n_ivf = min(int(16 * np.sqrt(big_npy.shape[0])), big_npy.shape[0] // 39)\n",
        "    infos.append(\"%s,%s\" % (big_npy.shape, n_ivf))\n",
        "    yield \"\\n\".join(infos)\n",
        "    index = faiss.index_factory(256 if version19 == \"v1\" else 768, \"IVF%s,Flat\" % n_ivf)\n",
        "    infos.append(\"training\")\n",
        "    yield \"\\n\".join(infos)\n",
        "    index_ivf = faiss.extract_index_ivf(index)  #\n",
        "    index_ivf.nprobe = 1\n",
        "    index.train(big_npy)\n",
        "    faiss.write_index(\n",
        "        index,\n",
        "        \"%s/trained_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (exp_dir, n_ivf, index_ivf.nprobe, exp_dir1, version19),\n",
        "    )\n",
        "\n",
        "    infos.append(\"adding\")\n",
        "    yield \"\\n\".join(infos)\n",
        "    batch_size_add = 8192\n",
        "    for i in range(0, big_npy.shape[0], batch_size_add):\n",
        "        index.add(big_npy[i : i + batch_size_add])\n",
        "    faiss.write_index(\n",
        "        index,\n",
        "        \"%s/added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (exp_dir, n_ivf, index_ivf.nprobe, exp_dir1, version19),\n",
        "    )\n",
        "    infos.append(\n",
        "        \"成功构建索引，added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (n_ivf, index_ivf.nprobe, exp_dir1, version19)\n",
        "    )\n",
        "\n",
        "training_log = train_index(model_name, 'v2')\n",
        "\n",
        "for line in training_log:\n",
        "    print(line)\n",
        "    if 'adding' in line:\n",
        "        clear_output()\n",
        "        display(Button(description=\"\\u2714 Success\", button_style=\"success\"))"
      ],
      "metadata": {
        "id": "3KyMRbK49g__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.Train Model\n",
        "#@markdown <small> Enter your ngrok authtoken to open tensorboard. Get one here: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "ngrok_authtoken = \"2OkBvQxPQw7QeiCZMUlszGMX4Jq_2LLKJDWYr1WG7nGRLD85R\"#@param {type:\"string\"}\n",
        "!ngrok config add-authtoken {ngrok_authtoken}\n",
        "#%cd /content/drive/MyDrive/project-main\n",
        "from random import shuffle\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "from pyngrok import ngrok\n",
        "now_dir=os.getcwd()\n",
        "#@markdown <small> Enter the name of your model again. It must be the same you chose before.\n",
        "model_name = 'elon'#@param {type:\"string\"}\n",
        "#@markdown <small> Choose how often to save the model and how much training you want it to have.\n",
        "save_frequency = 20 # @param {type:\"slider\", min:5, max:50, step:5}\n",
        "epochs = 300 # @param {type:\"slider\", min:10, max:1000, step:10}\n",
        "#@markdown <small> ONLY cache datasets under 10 minutes long. Otherwise leave this unchecked.\n",
        "cache = True #@param {type:\"boolean\"}\n",
        "# Remove the logging setup\n",
        "\n",
        "def click_train(\n",
        "    exp_dir1,\n",
        "    sr2,\n",
        "    if_f0_3,\n",
        "    spk_id5,\n",
        "    save_epoch10,\n",
        "    total_epoch11,\n",
        "    batch_size12,\n",
        "    if_save_latest13,\n",
        "    pretrained_G14,\n",
        "    pretrained_D15,\n",
        "    gpus16,\n",
        "    if_cache_gpu17,\n",
        "    if_save_every_weights18,\n",
        "    version19,\n",
        "):\n",
        "    # 生成filelist\n",
        "    exp_dir = \"%s/logs/%s\" % (now_dir, exp_dir1)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "    gt_wavs_dir = \"%s/0_gt_wavs\" % (exp_dir)\n",
        "    feature_dir = (\n",
        "        \"%s/3_feature256\" % (exp_dir)\n",
        "        if version19 == \"v1\"\n",
        "        else \"%s/3_feature768\" % (exp_dir)\n",
        "    )\n",
        "    if if_f0_3:\n",
        "        f0_dir = \"%s/2a_f0\" % (exp_dir)\n",
        "        f0nsf_dir = \"%s/2b-f0nsf\" % (exp_dir)\n",
        "        names = (\n",
        "            set([name.split(\".\")[0] for name in os.listdir(gt_wavs_dir)])\n",
        "            & set([name.split(\".\")[0] for name in os.listdir(feature_dir)])\n",
        "            & set([name.split(\".\")[0] for name in os.listdir(f0_dir)])\n",
        "            & set([name.split(\".\")[0] for name in os.listdir(f0nsf_dir)])\n",
        "        )\n",
        "    else:\n",
        "        names = set([name.split(\".\")[0] for name in os.listdir(gt_wavs_dir)]) & set(\n",
        "            [name.split(\".\")[0] for name in os.listdir(feature_dir)]\n",
        "        )\n",
        "    opt = []\n",
        "    for name in names:\n",
        "        if if_f0_3:\n",
        "            opt.append(\n",
        "                \"%s/%s.wav|%s/%s.npy|%s/%s.wav.npy|%s/%s.wav.npy|%s\"\n",
        "                % (\n",
        "                    gt_wavs_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    feature_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    f0_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    f0nsf_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    spk_id5,\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            opt.append(\n",
        "                \"%s/%s.wav|%s/%s.npy|%s\"\n",
        "                % (\n",
        "                    gt_wavs_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    feature_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    spk_id5,\n",
        "                )\n",
        "            )\n",
        "    fea_dim = 256 if version19 == \"v1\" else 768\n",
        "    if if_f0_3:\n",
        "        for _ in range(2):\n",
        "            opt.append(\n",
        "                \"%s/logs/mute/0_gt_wavs/mute%s.wav|%s/logs/mute/3_feature%s/mute.npy|%s/logs/mute/2a_f0/mute.wav.npy|%s/logs/mute/2b-f0nsf/mute.wav.npy|%s\"\n",
        "                % (now_dir, sr2, now_dir, fea_dim, now_dir, now_dir, spk_id5)\n",
        "            )\n",
        "    else:\n",
        "        for _ in range(2):\n",
        "            opt.append(\n",
        "                \"%s/logs/mute/0_gt_wavs/mute%s.wav|%s/logs/mute/3_feature%s/mute.npy|%s\"\n",
        "                % (now_dir, sr2, now_dir, fea_dim, spk_id5)\n",
        "            )\n",
        "    shuffle(opt)\n",
        "    with open(\"%s/filelist.txt\" % exp_dir, \"w\") as f:\n",
        "        f.write(\"\\n\".join(opt))\n",
        "\n",
        "    # Replace logger.debug, logger.info with print statements\n",
        "    print(\"Write filelist done\")\n",
        "    print(\"Use gpus:\", str(gpus16))\n",
        "    if pretrained_G14 == \"\":\n",
        "        print(\"No pretrained Generator\")\n",
        "    if pretrained_D15 == \"\":\n",
        "        print(\"No pretrained Discriminator\")\n",
        "    if version19 == \"v1\" or sr2 == \"40k\":\n",
        "        config_path = \"configs/v1/%s.json\" % sr2\n",
        "    else:\n",
        "        config_path = \"configs/v2/%s.json\" % sr2\n",
        "    config_save_path = os.path.join(exp_dir, \"config.json\")\n",
        "    if not pathlib.Path(config_save_path).exists():\n",
        "        with open(config_save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            with open(config_path, \"r\") as config_file:\n",
        "                config_data = json.load(config_file)\n",
        "                json.dump(\n",
        "                    config_data,\n",
        "                    f,\n",
        "                    ensure_ascii=False,\n",
        "                    indent=4,\n",
        "                    sort_keys=True,\n",
        "                )\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    cmd = (\n",
        "        'python infer/modules/train/train.py -e \"%s\" -sr %s -f0 %s -bs %s -g %s -te %s -se %s %s %s -l %s -c %s -sw %s -v %s'\n",
        "        % (\n",
        "            exp_dir1,\n",
        "            sr2,\n",
        "            1 if if_f0_3 else 0,\n",
        "            batch_size12,\n",
        "            gpus16,\n",
        "            total_epoch11,\n",
        "            save_epoch10,\n",
        "            \"-pg %s\" % pretrained_G14 if pretrained_G14 != \"\" else \"\",\n",
        "            \"-pd %s\" % pretrained_D15 if pretrained_D15 != \"\" else \"\",\n",
        "            1 if if_save_latest13 == True else 0,\n",
        "            1 if if_cache_gpu17 == True else 0,\n",
        "            1 if if_save_every_weights18 == True else 0,\n",
        "            version19,\n",
        "        )\n",
        "    )\n",
        "    # Use PIPE to capture the output and error streams\n",
        "    p = Popen(cmd, shell=True, cwd=now_dir, stdout=PIPE, stderr=STDOUT, bufsize=1, universal_newlines=True)\n",
        "\n",
        "    # Print the command's output as it runs\n",
        "    for line in p.stdout:\n",
        "        print(line.strip())\n",
        "\n",
        "    # Wait for the process to finish\n",
        "    p.wait()\n",
        "    return \"训练结束, 您可查看控制台训练日志或实验文件夹下的train.log\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs --port=8888\n",
        "print(\"Tensorboard NGROK URL:\",end=\"\")\n",
        "try:\n",
        "    training_log = click_train(\n",
        "        model_name,\n",
        "        '40k',\n",
        "        True,\n",
        "        0,\n",
        "        save_frequency,\n",
        "        epochs,\n",
        "        12,\n",
        "        True,\n",
        "        'assets/pretrained_v2/f0G40k.pth',\n",
        "        'assets/pretrained_v2/f0D40k.pth',\n",
        "        0,\n",
        "        cache,\n",
        "        True,\n",
        "        'v2',\n",
        "    )\n",
        "    print(training_log)\n",
        "except:\n",
        "    ngrok.kill()"
      ],
      "metadata": {
        "id": "FFfC9x239kC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inference\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "transpose = 0#@param {type:\"integer\"}\n",
        "input_path = \"/content/drive/MyDrive/harvard.wav\"#@param {type:\"string\"}\n",
        "index_path = \"/logs/elon/added_IVF749_Flat_nprobe_1_elon_v2.index\"#@param {type:\"string\"}\n",
        "f0_method = \"rmvpe\" # @param [\"rmvpe\", \"pm\", \"harvest\"]\n",
        "opt_path = \"/audios/cli_output.wav\"#@param {type:\"string\"}\n",
        "model_name = \"elon.pth\"#@param {type:\"string\"}\n",
        "index_rate = 0.63 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "volume_normalization = 0.13 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "consonant_protection = 0.33 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "#!rm $opt_path\n",
        "!python tools/infer_cli.py --f0up_key $transpose \\\n",
        "--input_path $input_path \\\n",
        "--index_path $index_path \\\n",
        "--f0method $f0_method \\\n",
        "--opt_path $opt_path \\\n",
        "--model_name $model_name \\\n",
        "--index_rate $index_rate \\\n",
        "--device cuda:0 \\\n",
        "--is_half True \\\n",
        "--filter_radius 3 \\\n",
        "--resample_sr 0 \\\n",
        "--rms_mix_rate $volume_normalization \\\n",
        "--protect $consonant_protection\n",
        "\n",
        "import IPython.display as ipd\n",
        "\n",
        "#ipd.clear_output()\n",
        "sample_rate = 40000\n",
        "\n",
        "ipd.Audio(opt_path, rate=sample_rate)\n"
      ],
      "metadata": {
        "id": "Fz3XSI8GrXra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import sys\n",
        "import wave\n",
        "from pathlib import Path\n",
        "\n",
        "BASE_DIR = Path('/content')\n",
        "sys.path.append(str(BASE_DIR))\n",
        "\n",
        "import torch\n",
        "from multiprocessing import cpu_count\n",
        "from flask_ngrok3 import run_with_ngrok\n",
        "from flask import Flask, request, send_file\n",
        "\n",
        "from vc_infer_pipeline import VC\n",
        "from infer_pack.models import (\n",
        "    SynthesizerTrnMs256NSFsid,\n",
        "    SynthesizerTrnMs256NSFsid_nono,\n",
        "    SynthesizerTrnMs768NSFsid,\n",
        "    SynthesizerTrnMs768NSFsid_nono,\n",
        ")\n",
        "from my_utils import load_audio\n",
        "from fairseq import checkpoint_utils\n",
        "from scipy.io import wavfile\n",
        "\n",
        "\n",
        "INPUT_VOICE_PATH = 'input.mp3'\n",
        "OUTPUT_VOICE_PATH = 'output.wav'\n",
        "MODEL_NAME = 'elon'\n",
        "DEVICE = 'cuda:0'\n",
        "cpt = None\n",
        "\n",
        "\n",
        "class Config:\n",
        "  def __init__(self, device, is_half):\n",
        "    self.device = device\n",
        "    self.is_half = is_half\n",
        "    self.n_cpu = 0\n",
        "    self.gpu_name = None\n",
        "    self.gpu_mem = None\n",
        "    self.x_pad, self.x_query, self.x_center, self.x_max = self.device_config()\n",
        "\n",
        "  def device_config(self) -> tuple:\n",
        "    if torch.cuda.is_available():\n",
        "      i_device = int(self.device.split(\":\")[-1])\n",
        "      self.gpu_name = torch.cuda.get_device_name(i_device)\n",
        "\n",
        "      if (\n",
        "        (\"16\" in self.gpu_name and \"V100\" not in self.gpu_name.upper())\n",
        "        or \"P40\" in self.gpu_name.upper()\n",
        "        or \"1060\" in self.gpu_name\n",
        "        or \"1070\" in self.gpu_name\n",
        "        or \"1080\" in self.gpu_name\n",
        "      ):\n",
        "        print(\"16 series/10 series P40 forced single precision\")\n",
        "        self.is_half = False\n",
        "        for config_file in [\"32k.json\", \"40k.json\", \"48k.json\"]:\n",
        "          with open(f\"configs/{config_file}\", \"r\") as f:\n",
        "            strr = f.read().replace(\"true\", \"false\")\n",
        "          with open(f\"configs/{config_file}\", \"w\") as f:\n",
        "            f.write(strr)\n",
        "        with open(\"trainset_preprocess_pipeline_print.py\", \"r\") as f:\n",
        "          strr = f.read().replace(\"3.7\", \"3.0\")\n",
        "        with open(\"trainset_preprocess_pipeline_print.py\", \"w\") as f:\n",
        "          f.write(strr)\n",
        "      else:\n",
        "        self.gpu_name = None\n",
        "\n",
        "      self.gpu_mem = int(\n",
        "        torch.cuda.get_device_properties(i_device).total_memory\n",
        "        / 1024\n",
        "        / 1024\n",
        "        / 1024\n",
        "        + 0.4\n",
        "      )\n",
        "      if self.gpu_mem <= 4:\n",
        "        with open(\"trainset_preprocess_pipeline_print.py\", \"r\") as f:\n",
        "          strr = f.read().replace(\"3.7\", \"3.0\")\n",
        "        with open(\"trainset_preprocess_pipeline_print.py\", \"w\") as f:\n",
        "          f.write(strr)\n",
        "\n",
        "    elif torch.backends.mps.is_available():\n",
        "      print(\"No supported N-card found, use MPS for inference\")\n",
        "      self.device = \"mps\"\n",
        "    else:\n",
        "      print(\"No supported N-card found, use CPU for inference\")\n",
        "      self.device = \"cpu\"\n",
        "      self.is_half = True\n",
        "\n",
        "    if self.n_cpu == 0:\n",
        "      self.n_cpu = cpu_count()\n",
        "\n",
        "    if self.is_half:\n",
        "      # 6G memory config\n",
        "      x_pad = 3\n",
        "      x_query = 10\n",
        "      x_center = 60\n",
        "      x_max = 65\n",
        "    else:\n",
        "      # 5G memory config\n",
        "      x_pad = 1\n",
        "      x_query = 6\n",
        "      x_center = 38\n",
        "      x_max = 41\n",
        "\n",
        "    if self.gpu_mem != None and self.gpu_mem <= 4:\n",
        "      x_pad = 1\n",
        "      x_query = 5\n",
        "      x_center = 30\n",
        "      x_max = 32\n",
        "\n",
        "    return x_pad, x_query, x_center, x_max\n",
        "\n",
        "\n",
        "CONFIG = Config(DEVICE, True)\n",
        "\n",
        "\n",
        "def load_hubert():\n",
        "  models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task(['hubert_base.pt'], suffix='', )\n",
        "  hubert = models[0]\n",
        "  hubert = hubert.to(DEVICE)\n",
        "\n",
        "  if True:\n",
        "    hubert = hubert.half()\n",
        "  else:\n",
        "    hubert = hubert.float()\n",
        "\n",
        "  hubert.eval()\n",
        "  return hubert\n",
        "\n",
        "HUBERT_MODEL = load_hubert()\n",
        "\n",
        "\n",
        "def get_vc(device, is_half, config):\n",
        "  global cpt, version, net_g, tgt_sr, vc\n",
        "  model_path = BASE_DIR / 'weights' / f'{MODEL_NAME}.pth'\n",
        "  if not model_path.exists():\n",
        "    print(f'The model {model_path} does not exist. Please ensure that you have filled in the proper MODEL_NAME in your .env file.')\n",
        "    return None\n",
        "\n",
        "  model_path = str(model_path)\n",
        "  print(f'loading pth {model_path}')\n",
        "  cpt = torch.load(model_path, map_location='cpu')\n",
        "  tgt_sr = cpt[\"config\"][-1]\n",
        "  cpt[\"config\"][-3] = cpt[\"weight\"][\"emb_g.weight\"].shape[0]\n",
        "  if_f0 = cpt.get(\"f0\", 1)\n",
        "  version = cpt.get(\"version\", \"v1\")\n",
        "\n",
        "  if version == \"v1\":\n",
        "    if if_f0 == 1:\n",
        "      net_g = SynthesizerTrnMs256NSFsid(*cpt[\"config\"], is_half=is_half)\n",
        "    else:\n",
        "      net_g = SynthesizerTrnMs256NSFsid_nono(*cpt[\"config\"])\n",
        "  elif version == \"v2\":\n",
        "    if if_f0 == 1:\n",
        "      net_g = SynthesizerTrnMs768NSFsid(*cpt[\"config\"], is_half=is_half)\n",
        "    else:\n",
        "      net_g = SynthesizerTrnMs768NSFsid_nono(*cpt[\"config\"])\n",
        "\n",
        "  del net_g.enc_q\n",
        "  print(net_g.load_state_dict(cpt[\"weight\"], strict=False))\n",
        "  net_g.eval().to(device)\n",
        "\n",
        "  if is_half:\n",
        "    net_g = net_g.half()\n",
        "  else:\n",
        "    net_g = net_g.float()\n",
        "\n",
        "  vc = VC(tgt_sr, config)\n",
        "\n",
        "\n",
        "def rvc_infer(pitch_change, pitch_extraction_algo, volume_envelope, index_rate):\n",
        "  logs_dir = BASE_DIR / 'logs' / MODEL_NAME\n",
        "  index_path = ''\n",
        "  for file in logs_dir.iterdir():\n",
        "    if file.suffix == '.index':\n",
        "      index_path = str(logs_dir / file.name)\n",
        "      break\n",
        "\n",
        "  # vc single\n",
        "  audio = load_audio(INPUT_VOICE_PATH, 16000)\n",
        "  times = [0, 0, 0]\n",
        "  if_f0 = cpt.get('f0', 1)\n",
        "  audio_opt = vc.pipeline(HUBERT_MODEL, net_g, 0, audio, INPUT_VOICE_PATH, times, pitch_change, pitch_extraction_algo, index_path, index_rate, if_f0, 3, tgt_sr, 0, volume_envelope, version, 0.33, f0_file=None)\n",
        "  wavfile.write(OUTPUT_VOICE_PATH, tgt_sr, audio_opt)\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app, auth_token=NGROK_AUTH_TOKEN)\n",
        "\n",
        "@app.route('/', methods=['GET'])\n",
        "def test():\n",
        "  response = {'status':'OK','message':'Test'}\n",
        "  return json.dumps(response)\n",
        "\n",
        "\n",
        "@app.route('/infer', methods=['POST'])\n",
        "def infer():\n",
        "  global MODEL_NAME, cpt\n",
        "  model_name = request.args.get('model')\n",
        "  if MODEL_NAME != model_name:\n",
        "    MODEL_NAME = model_name\n",
        "    if cpt:\n",
        "      del cpt\n",
        "    get_vc(DEVICE, True, CONFIG)\n",
        "\n",
        "  pitch_change = int(request.args.get('pitch'))\n",
        "  pitch_extraction_algo = request.args.get('algo')\n",
        "  volume_envelope = float(request.args.get('volume'))\n",
        "  index_rate = float(request.args.get('index_rate'))\n",
        "  audio_data = request.files['audio_file']\n",
        "  audio_data.save(INPUT_VOICE_PATH)\n",
        "  rvc_infer(pitch_change, pitch_extraction_algo, volume_envelope, index_rate)\n",
        "  return send_file(OUTPUT_VOICE_PATH, mimetype=\"audio/wav\")\n",
        "\n",
        "app.run()"
      ],
      "metadata": {
        "id": "nLI0Ap0kKE9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OR Open the GUI (Banned for free Colab Notebooks)\n",
        "if not 'installed' in locals():\n",
        "    %cd /content\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    from IPython.display import clear_output\n",
        "    from ipywidgets import Button\n",
        "    import os\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Your drive is not mounted. Creating Fake Drive.\")\n",
        "        os.makedirs('/content/drive/MyDrive')\n",
        "    if not os.path.exists('/content/drive/MyDrive/project-main'):\n",
        "        !wget https://huggingface.co/Rejekts/project/resolve/main/project-main.zip -O '/content/project-main.zip' && unzip 'project-main.zip' -d /content/drive/MyDrive\n",
        "    !cd '/content/drive/MyDrive/project-main' && python download_files.py && pip install -r 'requirements.txt'\n",
        "    !rm /content/project-main.zip\n",
        "    !rm -r /content/sample_data\n",
        "    !mkdir -p /content/dataset\n",
        "    clear_output()\n",
        "    Button(description=\"\\u2714 Success\", button_style=\"success\")\n",
        "tensorboard = True #@param {type:\"boolean\"}\n",
        "if tensorboard:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir ./logs\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "!python app.py --colab"
      ],
      "metadata": {
        "id": "DZDKirCM0F9g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}